% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/confusion_metrics.R
\name{confusion_metrics}
\alias{confusion_metrics}
\title{Compute Confusion Matrix Metrics}
\usage{
confusion_metrics(y_true, y_pred, cutoff = 0.5)
}
\arguments{
\item{y_true}{A numeric vector containing the true class labels (binary: 0 or 1).}

\item{y_pred}{A numeric vector containing the predicted class labels (binary: 0 or 1).}

\item{cutoff}{Cut-off value for classification (default = 0.5).}
}
\value{
A list containing the following metrics:
\describe{
\item{prevalence}{Proportion of positive cases in the data.}
\item{accuracy}{Overall accuracy of the classification model.}
\item{sensitivity}{Proportion of true positives identified correctly.}
\item{specificity}{Proportion of true negatives identified correctly.}
\item{FDR}{False Discovery Rate: Proportion of predicted positives that are false.}
\item{DOR}{Diagnostic Odds Ratio: Ratio of the odds of positive prediction for true positives compared to false negatives.}
}
}
\description{
This function calculates various evaluation metrics from a confusion matrix
for a binary classification model, including accuracy, sensitivity, specificity,
false discovery rate, and diagnostic odds ratio.
}
\examples{
y_true <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 0)
y_pred <- c(1, 0, 1, 1, 0, 1, 1, 0, 1, 0)
metrics <- confusion_metrics(y_true, y_pred)
print(metrics)
}
\author{
Yuchen Wang
}
