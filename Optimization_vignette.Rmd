---
title: "Optimization Vignette"
output: html_document
date: "2024-12-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The `Optimization` package is developed based on implementing supervised binary classification using numerical optimization. The package contains functions that estimate the coefficient vector $\beta$, which includes the independent variables/predictors along with the intercept. The package then allows the user to obtain different outputs from the procedure. The estimator is computed using numerical optimization of the following: 

$$
\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n} \left(-y_i \cdot \ln(p_i) - (1 - y_i) \cdot \ln(1 - p_i)\right),
$$
where
$$
p_i = \frac{1}{1 + \exp(-x_i^\top \beta)},
$$
and $y_i$ and $x_i$ represent the $i^{th}$ observation and row of the response and predictors respectively. 

The basic outputs of the procedure are as follows: 

- The initial values of $\beta$ for the optimization procedure are found using the least-squares formula: $(X^T X)^{-1} X^T y$.
- The user is able to choose the significance level $\alpha$ for obtaining the $1 - \alpha$ confidence intervals for $\beta$ using the Bootstrap method, along with the number of bootstraps. The default number of bootstraps will be 20. 
- The confusion matrix will be obtained using a cut-off value for prediction at 0.5, with the following metrics to be returned based on the cut-off value: 
  - Prevalence
  - Accuracy
  - Sensitivity
  - Specificity
  - False Discovery Rate
  - Diagnostic Odds Ration
- Help documentation for all functions is available. 

To show examples of how to utilize this package, the adult.csv dataset will be used. 

## Data Pre-Processing

The dataset that will be used for this example is the adult.csv file from the STAT6210 class Canvas page. The following commands allow us to understand the dataset we are working with.

```{r}
adult <- read.csv("adult.csv", sep = ";")
head(adult)
nrow(adult)
```

As this dataset is so large, for run-time and simplicity, we will be using the marital status and race as the predictor/income variables, and sex as the binary response variable, where 1 = male and 0 = female. 

Since this dataset has character vectors, we need to convert these vectors to numeric using the function model.matrix. This function also creates a column of ones for the intercept.The functions that are in the package assume that the intercept column is already added, so the user needs to make sure this is true of their dataset. 


```{r}
adult[] <- lapply(adult, function(x) if (is.character(x)) as.factor(x) else x)
selected_columns <- adult[, c(7, 10, 11)] # subset the dataframe to include only columns 7, 10, and 11
formula <- ~ .  # this specifies how to model the data
design_matrix <- model.matrix(formula, data = selected_columns)
head(design_matrix)
dim(design_matrix)
```


## Installing the optimization package

The package is available for downloading from a GitHub repository named Final_Project_Group1 within the AU-R-Programming organization. 

```{r, message=FALSE,warning=FALSE}
library(devtools)
install_github("AU-R-Programming/Final_Project_Group1")

```

## Using the Optimization package
There are three functions included in this package, and their descriptions and examples of how to use them are found below. 

### opt_beta_est

The opt_beta_est function can be used to estimate the vector $\beta$ for our dataset based on the loss function provided above through minimizing the function. The response variable for this dataset is the `sex` column, which is column 12. 

```{r,message=FALSE}
library(Optimization)
X <- design_matrix[, -12]
y <- design_matrix[, 12]
dim(X) 
length(y)
beta_opt_func <- opt_beta_est(X, y)
print(beta_opt_func)
length(beta_opt_func)
```
This function returns the optimal estimated $\beta$ values for this dataset. 

The first element of $\beta$ corresponds to the intercept term, which represents the baseline log-odds of the outcome when all the predictors are zero. The other elements of $\beta$ are the weights of the predictors, where each coefficient measures the change in the log-odds of the outcome per unit increase in the corresponding predictor, assuming the other predictors are held constant. These coefficients provide a way to quantify the strength and direction of the relationship between each predictor and the log-odds of the outcome. For our example, married-civ-spouse is positive as the third element of $\beta$, indicating that as this predictor increases, the probability of a positive class increases as well. The opposite is true for the negative predictors. 

This optimized $\beta$ vector provides the intercept and coefficients for the logistic regression model that makes the model's predicted probabilities, $p_i$, closest to the actual outcome, $y_i$.

### bootstrap_ci

Another function within this optimization package is the `bootstrap_ci` function. This function uses the bootstrap method of re-sampling to create confidence intervals at the specified confidence level, $\alpha$. The number of times the bootstrap procedure is repeated can be specified by the user, with the default being 20. 

```{r}
bootstrap_ci(X, y, beta_opt_func, alpha = 0.05, n_bootstrap = 20)
```
The output of  this function is the $1 - \alpha$ confidence intervals, giving the upper and lower limits for each element of the $\beta$ vector. For this example, $\alpha = 0.05$, so the output corresponds to the 95% confidence level for each of the predictors. 


### confusion_metrics

The last function that is included in this `Optimization` package creates a confusion matrix. It is based on a cut-off value for prediction, which is 0.5.

```{r}
y_pred <- 1 / (1 + exp(-X %*% beta_opt_func))  # use design matrix with intercept  

# Calculate metrics
metrics <- confusion_metrics(y, y_pred, cutoff = 0.5)# print confusion matrix metrics
print(metrics)
```
The `confusion_metrics` function calculates various performance metrics for a binary classification model based on the true labels (\( y_{\text{true}} \)) and predicted probabilities (\( y_{\text{pred}} \)). Using a specified cutoff (default is 0.5), it converts probabilities into binary predictions and computes key metrics, including true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). From these, it derives metrics such as prevalence (proportion of positives in the data), accuracy, sensitivity (true positive rate), specificity (true negative rate), false discovery rate (FDR), and diagnostic odds ratio (DOR). The function returns these metrics as a list for model evaluation.




## References 
Help in creating this Optimization package: 

https://chatgpt.com/share/674f3319-5394-8000-aecf-6dcc60bd14a6

https://chatgpt.com/c/674ce4d0-c7ac-8011-a38d-c663f701e0c7

https://chatgpt.com/c/674cf199-9ff0-8011-a4ad-62071ca7eef5

https://chatgpt.com/share/674d1772-d064-8005-9b04-9679d24ff7d8




